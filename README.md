# Motivation for Image Captioning

To generate a gramatically correct sentence which can accurately describe the scene of an image, enabling any individual to visualize the image mentally. Instead of simply detecting objects, the network aims to establish a relationship among entities in the image.

# Block Diagram

![Block Diagram](https://github.com/hasnainroopawalla/Image-Captioning-Scene-Descriptor/blob/master/images/architecture.JPG)

# Image Captioning using Uni-Directional and Bi-Directional LSTM

Image Features are extracted using InceptionV3 model(Pretrained) 
Captioning Model is trained on the Flickr8k Dataset

Navigate to the 'Flickr' directory in the command prompt:
```python run.py```

The parent folder of this repository should contain the trained caption_model weights.

The Image Captioning Model is deployed as a REST API i.e., the web-app as well as our Flutter Application makes API calls to the server by sending an image and the server responds with a caption

The Web-App displays the Bidirectional as well as Uni directional Approaches side-by-side and also a table for the accuracy of each predicted word

# Model Architecture (Uni-Directional and Bi-Directional respectively)

![Model Architecture](https://github.com/hasnainroopawalla/Image-Captioning-Scene-Descriptor/blob/master/images/unibi.JPG)

# Test Results (Flask Web-App):

Football players:

![Football players](https://github.com/hasnainroopawalla/Image-Captioning-Scene-Descriptor/blob/master/images/Capture1.JPG)

Snowy Scene:

![Snowy Scene](https://github.com/hasnainroopawalla/Image-Captioning-Scene-Descriptor/blob/master/images/Capture2.JPG)

Running Dog:

![Running Dog](https://github.com/hasnainroopawalla/Image-Captioning-Scene-Descriptor/blob/master/images/Capture3.JPG)

Jumping Dog:

![Jumping Dog](https://github.com/hasnainroopawalla/Image-Captioning-Scene-Descriptor/blob/master/images/Capture4.JPG)

2 Running Dogs:

![2 Running Dogs](https://github.com/hasnainroopawalla/Image-Captioning-Scene-Descriptor/blob/master/images/Capture5.JPG)

# Test Results (Flutter App):

The Caption Model is deployed as a REST API locally on my laptop and the Flutter Application fetches data using it

Dog on Beach:

![Dog on Beach](https://github.com/hasnainroopawalla/Image-Captioning-Scene-Descriptor/blob/master/images/Capture8.jpeg)

3 Dogs:

![3 Dogs](https://github.com/hasnainroopawalla/Image-Captioning-Scene-Descriptor/blob/master/images/Capture9.jpeg)

Dog Jumping over Hurdle:

![Dog Jumping over Hurdle](https://github.com/hasnainroopawalla/Image-Captioning-Scene-Descriptor/blob/master/images/Capture10.jpeg)

Basketball Boy:

![Basketball Boy](https://github.com/hasnainroopawalla/Image-Captioning-Scene-Descriptor/blob/master/images/Capture11.jpeg)

# Evaluation Metric

The BLEU Metric has been used to evaluate the test images. A higher BLEU rating (closer to 1) corresponds to an accurate description

The captions are generated by the model on a Laptop CPU which results in a higher processing time
Deploying the model on the cloud can improve performance
